{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxtxOsiwNX/L36i9PNDyo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshiitp/Assignment-1-APR/blob/main/surjii.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff_pRqgOe2qT"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchtext pandas numpy scikit-learn nltk matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload your Flipkart CSV here\n"
      ],
      "metadata": {
        "id": "b3OOv_sPf0mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load and Preprocess Dataset (Safe Manual Version)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load CSV safely\n",
        "try:\n",
        "    df = pd.read_csv(list(uploaded.keys())[0], encoding='utf-8', errors='ignore')\n",
        "except:\n",
        "    df = pd.read_csv(list(uploaded.keys())[0], encoding='ISO-8859-1')\n",
        "\n",
        "print(\"CSV Loaded. Columns:\", df.columns)\n",
        "\n",
        "# ------------------- MANUAL COLUMN SELECTION -------------------\n",
        "review_col = 'Review'\n",
        "rating_col = 'Rate'\n",
        "\n",
        "# Basic cleaning of review text\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "    return text\n",
        "\n",
        "df['clean_review'] = df[review_col].apply(clean_text)\n",
        "\n",
        "# Convert ratings to sentiment labels\n",
        "def rating_to_sentiment(r):\n",
        "    try:\n",
        "        r = float(r)\n",
        "    except:\n",
        "        r = 3  # Treat invalid/missing ratings as Neutral\n",
        "    if r <= 2:\n",
        "        return 0  # Negative\n",
        "    elif r == 3:\n",
        "        return 1  # Neutral\n",
        "    else:\n",
        "        return 2  # Positive\n",
        "\n",
        "df['label'] = df[rating_col].apply(rating_to_sentiment)\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['clean_review'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", len(train_texts))\n",
        "print(\"Testing samples:\", len(test_texts))\n"
      ],
      "metadata": {
        "id": "xQQy8Nl3jWNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Corrected FNN for Bag-of-Words\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Dataset class\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vectorizer):\n",
        "        self.X = vectorizer.transform(texts).toarray()\n",
        "        self.y = labels.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "vectorizer.fit(train_texts)\n",
        "\n",
        "train_dataset = ReviewDataset(train_texts, train_labels, vectorizer)\n",
        "test_dataset = ReviewDataset(test_texts, test_labels, vectorizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Feed-Forward Network\n",
        "input_dim = len(vectorizer.vocabulary_)  # <-- Correct input size\n",
        "hidden_dim = 128\n",
        "output_dim = 3\n",
        "\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = FNN(input_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=['Negative','Neutral','Positive']))\n"
      ],
      "metadata": {
        "id": "0Q1A406YkzQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "a8CILi9bmUPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize reviews\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "tokenized_train = [tokenize(text) for text in train_texts]\n",
        "tokenized_test = [tokenize(text) for text in test_texts]\n",
        "\n",
        "# Build vocabulary manually\n",
        "from collections import Counter\n",
        "\n",
        "all_tokens = [token for review in tokenized_train for token in review]\n",
        "token_counts = Counter(all_tokens)\n",
        "vocab_tokens = [\"<unk>\"] + [t for t, c in token_counts.most_common(5000)]  # top 5000 words\n",
        "vocab = {word: idx for idx, word in enumerate(vocab_tokens)}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Encode reviews to indices\n",
        "def encode(texts):\n",
        "    encoded = []\n",
        "    for text in texts:\n",
        "        indices = [vocab.get(word, 0) for word in tokenize(text)]  # 0 = <unk>\n",
        "        encoded.append(torch.tensor(indices, dtype=torch.long))\n",
        "    return encoded\n",
        "\n",
        "train_encoded = encode(train_texts)\n",
        "test_encoded = encode(test_texts)\n"
      ],
      "metadata": {
        "id": "VZyeyy-XnAvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence  # <- add this line\n",
        "\n",
        "# Pad sequences\n",
        "train_padded = pad_sequence(train_encoded, batch_first=True)\n",
        "test_padded = pad_sequence(test_encoded, batch_first=True)\n",
        "\n",
        "# Dataset class\n",
        "class ReviewSeqDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = sequences\n",
        "        self.y = torch.tensor(labels.values, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ReviewSeqDataset(train_padded, train_labels)\n",
        "test_dataset = ReviewSeqDataset(test_padded, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "vhec64bSngbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
        "    def forward(self, lstm_out):\n",
        "        weights = torch.softmax(self.attn(lstm_out), dim=1)\n",
        "        output = torch.sum(weights * lstm_out, dim=1)\n",
        "        return output\n",
        "\n",
        "class BiLSTM_Attention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTM_Attention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out = self.attention(lstm_out)\n",
        "        out = self.fc(attn_out)\n",
        "        return out\n",
        "\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 3  # Negative, Neutral, Positive\n",
        "\n",
        "model = BiLSTM_Attention(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "UhJYgaRXnmJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "XnSyH4YlnxBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=['Negative','Neutral','Positive']))\n"
      ],
      "metadata": {
        "id": "o1ZdLMzboDqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bilstm_attention_model.pth\")\n",
        "print(\"BiLSTM + Attention model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "CpyBGuaUoLXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch-geometric -q"
      ],
      "metadata": {
        "id": "o4CGFVJGT6CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch_geometric\n",
        "print(torch.__version__)\n",
        "print(torch_geometric.__version__)\n"
      ],
      "metadata": {
        "id": "e9Uzbpiq44rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrz6wb8T_jdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "vQE5P2Ix_kBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000  # or your actual vocab size\n",
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "output_dim = 3  # Negative, Neutral, Positive\n"
      ],
      "metadata": {
        "id": "ebH-r9DX_rCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "agMKJXfc_vCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMAttention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_dim*2, 1)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, hidden*2)\n",
        "        attn_weights = F.softmax(self.attention(lstm_out), dim=1)  # (batch, seq_len, 1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch, hidden*2)\n",
        "        out = self.fc(context)  # (batch, output_dim)\n",
        "        return out"
      ],
      "metadata": {
        "id": "GicMpdnh0ARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTMAttention(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "MJkJgmoh0Aje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "NTry6Yfx_xD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "n0fFtGWzBbSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Step 9: Convert padded sequences and labels to tensors\n",
        "# Use the variables defined in the previously executed cells.\n",
        "X_train_tensor = train_padded.clone().detach()\n",
        "y_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "X_test_tensor = test_padded.clone().detach()\n",
        "y_test_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "# Step 9: Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Step 9: Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "f1HSxlMw6pwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Evaluate the model\n",
        "model.eval()  # set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "soK9EBAj8O4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class BiLSTM_Attn_GNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTM_Attn_GNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # GNN Layer (Graph Convolution)\n",
        "        self.gcn1 = GCNConv(hidden_dim * 2, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index=None):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        # Attention\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        attn_out = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "\n",
        "        # GNN part\n",
        "        if edge_index is not None:\n",
        "            attn_out = self.gcn1(attn_out, edge_index)\n",
        "\n",
        "        out = self.fc(attn_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "pgR4KxGO87d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Combine BiLSTM with GNN (Graph Neural Network)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class BiLSTM_GNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTM_GNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.gcn1 = GCNConv(hidden_dim * 2, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = lstm_out.mean(dim=1)  # Global average pooling\n",
        "        x = self.relu(self.gcn1(x, edge_index))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "gnn_model = BiLSTM_GNN(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "print(\"✅ BiLSTM-GNN model is ready.\")\n"
      ],
      "metadata": {
        "id": "Lobx2wdR9Ka5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Memory-efficient graph creation\n",
        "import torch\n",
        "\n",
        "# Limit number of nodes to avoid RAM crash\n",
        "subset_size = min(500, X_train_tensor.size(0))  # take up to 500 samples\n",
        "X_subset = X_train_tensor[:subset_size]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create dummy graph edges using nearest neighbors\n",
        "# Here we just connect each node to the next 5 nodes to reduce edges\n",
        "edges = []\n",
        "k = 5  # number of neighbors per node\n",
        "for i in range(subset_size):\n",
        "    for j in range(1, k+1):\n",
        "        if i + j < subset_size:\n",
        "            edges.append([i, i+j])\n",
        "            edges.append([i+j, i])\n",
        "\n",
        "# Convert to torch tensor\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().to(device)  # shape [2, num_edges]\n",
        "\n",
        "print(\"Subset size (nodes):\", subset_size)\n",
        "print(\"Edge index shape:\", edge_index.shape)\n"
      ],
      "metadata": {
        "id": "dg9wW3pIdn-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Model Training (with memory-efficient graph edges)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Move model to device\n",
        "model = BiLSTMAttention(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5  # start with small number to check\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss\n"
      ],
      "metadata": {
        "id": "8QE5orjjiSWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Model Evaluation\n",
        "model.eval()  # set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "5DugR5eSf8mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Save the trained model\n",
        "model_path = \"bilstm_attention_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "rLV2gxE6lmtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 17: Inference on new data\n",
        "def predict_sentiment(text, vocab, model, device):\n",
        "    # Tokenize text\n",
        "    tokens = text.lower().split()  # simple split; you can use same tokenizer as training\n",
        "    # Encode tokens using your vocab\n",
        "    encoded = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "    # Convert to tensor and pad\n",
        "    tensor_input = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor_input)\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    labels = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return labels[pred]\n"
      ],
      "metadata": {
        "id": "XKBXzJ6Blo3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_text= \"Very happy with the purchase, highly recommend it.\"\n",
        "neutral_text = \"Average quality, nothing special or disappointing.\"\n",
        "negative_text =\"Very poor quality, broke after first use\""
      ],
      "metadata": {
        "id": "a1o0k_D6mp8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "sample_text = negative_text\n",
        "print(\"Predicted Sentiment:\", predict_sentiment(sample_text, vocab, model, device))"
      ],
      "metadata": {
        "id": "BYr2TEJHlzD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bilstm_attention_model.pth')\n"
      ],
      "metadata": {
        "id": "OVjz3wl0vRnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}